{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导入工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mAE_h7XhPT7d",
    "outputId": "83bf0f8e-fc69-40b1-f9fe-0025724a217c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import os.path as osp\n",
    "from tqdm import tqdm\n",
    "\n",
    "import mmcv\n",
    "import mmengine\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集图片和标注路径\n",
    "data_root = 'Glomeruli-dataset'\n",
    "img_dir = 'images'\n",
    "ann_dir = 'masks'\n",
    "\n",
    "# 类别和对应的颜色\n",
    "classes = ('background', 'glomeruili')\n",
    "palette = [[128, 128, 128], [151, 189, 8]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 修改数据集类（指定图像扩展名）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HchvmGYB_rrO"
   },
   "source": [
    "After downloading the data, we need to implement `load_annotations` function in the new dataset class `StanfordBackgroundDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "LbsWOw62_o-X"
   },
   "outputs": [],
   "source": [
    "from mmseg.registry import DATASETS\n",
    "from mmseg.datasets import BaseSegDataset\n",
    "\n",
    "@DATASETS.register_module()\n",
    "class StanfordBackgroundDataset(BaseSegDataset):\n",
    "  METAINFO = dict(classes = classes, palette = palette)\n",
    "  def __init__(self, **kwargs):\n",
    "    super().__init__(img_suffix='.png', seg_map_suffix='.png', **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文档：https://github.com/open-mmlab/mmsegmentation/blob/master/docs/en/tutorials/customize_datasets.md#customize-datasets-by-reorganizing-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUVtmn3Iq3WA"
   },
   "source": [
    "## 修改config配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing pspnet_r50-d8_4xb2-40k_cityscapes-512x1024...\n",
      "\u001B[32mpspnet_r50-d8_512x1024_40k_cityscapes_20200605_003338-2966598c.pth exists in /home/featurize/work/MMSegmentation_Tutorials-main/20230215/【D1】Kaggle代码实战-肾小球切片语义分割\u001B[0m\n",
      "\u001B[32mSuccessfully dumped pspnet_r50-d8_4xb2-40k_cityscapes-512x1024.py to /home/featurize/work/MMSegmentation_Tutorials-main/20230215/【D1】Kaggle代码实战-肾小球切片语义分割\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "# 下载 config 文件 和 预训练模型checkpoint权重文件\n",
    "!mim download mmsegmentation --config pspnet_r50-d8_4xb2-40k_cityscapes-512x1024 --dest ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Wwnj9tRzqX_A"
   },
   "outputs": [],
   "source": [
    "from mmengine import Config\n",
    "cfg = Config.fromfile('./mmsegmentation/configs/pspnet/pspnet_r50-d8_4xb2-40k_cityscapes-512x1024.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyKnYC1Z7iCV",
    "outputId": "6195217b-187f-4675-994b-ba90d8bb3078"
   },
   "outputs": [],
   "source": [
    "cfg.norm_cfg = dict(type='BN', requires_grad=True) # 只使用GPU时，BN取代SyncBN\n",
    "cfg.crop_size = (256, 256)\n",
    "cfg.model.data_preprocessor.size = cfg.crop_size\n",
    "cfg.model.backbone.norm_cfg = cfg.norm_cfg\n",
    "cfg.model.decode_head.norm_cfg = cfg.norm_cfg\n",
    "cfg.model.auxiliary_head.norm_cfg = cfg.norm_cfg\n",
    "# modify num classes of the model in decode/auxiliary head\n",
    "cfg.model.decode_head.num_classes = 2\n",
    "cfg.model.auxiliary_head.num_classes = 2\n",
    "\n",
    "# 修改数据集的 type 和 root\n",
    "cfg.dataset_type = 'StanfordBackgroundDataset'\n",
    "cfg.data_root = data_root\n",
    "\n",
    "cfg.train_dataloader.batch_size = 8\n",
    "\n",
    "cfg.train_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(type='LoadAnnotations'),\n",
    "    dict(type='RandomResize', scale=(320, 240), ratio_range=(0.5, 2.0), keep_ratio=True),\n",
    "    dict(type='RandomCrop', crop_size=cfg.crop_size, cat_max_ratio=0.75),\n",
    "    dict(type='RandomFlip', prob=0.5),\n",
    "    dict(type='PackSegInputs')\n",
    "]\n",
    "\n",
    "cfg.test_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(type='Resize', scale=(320, 240), keep_ratio=True),\n",
    "    # add loading annotation after ``Resize`` because ground truth\n",
    "    # does not need to do resize data transform\n",
    "    dict(type='LoadAnnotations'),\n",
    "    dict(type='PackSegInputs')\n",
    "]\n",
    "\n",
    "\n",
    "cfg.train_dataloader.dataset.type = cfg.dataset_type\n",
    "cfg.train_dataloader.dataset.data_root = cfg.data_root\n",
    "cfg.train_dataloader.dataset.data_prefix = dict(img_path=img_dir, seg_map_path=ann_dir)\n",
    "cfg.train_dataloader.dataset.pipeline = cfg.train_pipeline\n",
    "cfg.train_dataloader.dataset.ann_file = 'splits/train.txt'\n",
    "\n",
    "cfg.val_dataloader.dataset.type = cfg.dataset_type\n",
    "cfg.val_dataloader.dataset.data_root = cfg.data_root\n",
    "cfg.val_dataloader.dataset.data_prefix = dict(img_path=img_dir, seg_map_path=ann_dir)\n",
    "cfg.val_dataloader.dataset.pipeline = cfg.test_pipeline\n",
    "cfg.val_dataloader.dataset.ann_file = 'splits/val.txt'\n",
    "\n",
    "cfg.test_dataloader = cfg.val_dataloader\n",
    "\n",
    "# 验证 Evaluator\n",
    "# val_evaluator = dict(type='IoUMetric', iou_metrics=['mIoU', 'mDice', 'mFscore'])\n",
    "val_evaluator = dict(\n",
    "    type='IoUMetric',\n",
    "    iou_metrics=['mIoU', 'mDice', 'mFscore'],\n",
    "    output_dir='val_metrics',  # 单独保存验证结果\n",
    "    format_only=False,\n",
    "    print_results=True  # 强制打印所有结果\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------ 新增数据集元信息 ------------------\n",
    "cfg.metainfo = {\n",
    "    'classes': ('background', 'glomeruili'),  # 必须与您的标注类别一致\n",
    "    'palette': [[0, 0, 0], [255, 0, 0]]     # 颜色配置（可选）\n",
    "}\n",
    "\n",
    "# ------------------ 修改评估器配置 ------------------\n",
    "cfg.val_evaluator = dict(\n",
    "    type='IoUMetric',\n",
    "    iou_metrics=['mIoU', 'mDice', 'mFscore'],  # 明确指定所有指标\n",
    "    output_dir='val_metrics',                  # 评估结果保存路径\n",
    "    format_only=False,\n",
    "    # 关键：注入数据集类别信息\n",
    "    dataset_meta=cfg.metainfo  \n",
    ")\n",
    "\n",
    "# ------------------ 将metainfo注入所有数据集 ------------------\n",
    "for ds_key in ['train_dataloader', 'val_dataloader', 'test_dataloader']:\n",
    "    cfg[ds_key]['dataset']['metainfo'] = cfg.metainfo\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 测试 Evaluator\n",
    "test_evaluator = val_evaluator\n",
    "\n",
    "\n",
    "# 载入预训练模型权重\n",
    "cfg.load_from = 'pspnet_r50-d8_512x1024_40k_cityscapes_20200605_003338-2966598c.pth'\n",
    "\n",
    "# 工作目录\n",
    "cfg.work_dir = './work_dirs/tutorial'\n",
    "\n",
    "\n",
    "cfg.default_hooks = dict(\n",
    "    checkpoint=dict(\n",
    "        type='CheckpointHook',\n",
    "        interval = 400 ,\n",
    "        by_epoch=False,\n",
    "        filename_tmpl='CE{}.pth',  # 文件名模板（{} 会被替换为 epoch 或 iter）\n",
    "        save_best='mIoU'\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# 训练总迭代次数\n",
    "cfg.train_cfg.max_iters = 24000  \n",
    "\n",
    "# 验证频率 (每N次迭代)\n",
    "cfg.train_cfg.val_interval = 400  # 约每2个epoch验证一次\n",
    "\n",
    "# 日志记录频率 \n",
    "#cfg.default_hooks.logger.interval = 50  # 每50iter记录一次\n",
    "\n",
    "# 模型保存频率\n",
    "#cfg.default_hooks.checkpoint.interval = 800  # 每800iter保存一次\n",
    "\n",
    "# 随机数种子\n",
    "cfg['randomness'] = dict(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/featurize/work/MMSegmentation_Tutorials-main/20230215/mmsegmentation/mmseg/models/decode_heads/decode_head.py:120: UserWarning: For binary segmentation, we suggest using`out_channels = 1` to define the outputchannels of segmentor, and use `threshold`to convert `seg_logits` into a predictionapplying a threshold\n",
      "  warnings.warn('For binary segmentation, we suggest using'\n",
      "/home/featurize/work/MMSegmentation_Tutorials-main/20230215/mmsegmentation/mmseg/models/losses/cross_entropy_loss.py:251: UserWarning: Default ``avg_non_ignore`` is False, if you would like to ignore the certain label and average loss over non-ignore labels, which is the same with PyTorch official cross_entropy, set ``avg_non_ignore=True``.\n",
      "  'Default ``avg_non_ignore`` is False, if you would like to '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前评估器配置： {'type': 'IoUMetric', 'iou_metrics': ['mIoU', 'mDice', 'mFscore'], 'output_dir': 'val_metrics', 'format_only': False, 'dataset_meta': {'classes': ('background', 'glomeruili'), 'palette': [[0, 0, 0], [255, 0, 0]]}}\n"
     ]
    }
   ],
   "source": [
    "from mmseg.apis import init_model\n",
    "model = init_model(cfg, device='cuda')\n",
    "\n",
    "# 打印评估器配置\n",
    "print(\"当前评估器配置：\", cfg.val_evaluator)\n",
    "# 应输出包含 'mDice' 和 'mFscore' 的配置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from mmengine.runner import Runner\n",
    "#runner = Runner.from_cfg(cfg)\n",
    "#print(\"验证集类别信息：\", runner.val_dataloader.dataset.metainfo)\n",
    "# 应输出：{'classes': ['background', 'glomeruli'], ...}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 查看完整config配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crop_size = (\n",
      "    256,\n",
      "    256,\n",
      ")\n",
      "data_preprocessor = dict(\n",
      "    bgr_to_rgb=True,\n",
      "    mean=[\n",
      "        123.675,\n",
      "        116.28,\n",
      "        103.53,\n",
      "    ],\n",
      "    pad_val=0,\n",
      "    seg_pad_val=255,\n",
      "    size=(\n",
      "        512,\n",
      "        1024,\n",
      "    ),\n",
      "    std=[\n",
      "        58.395,\n",
      "        57.12,\n",
      "        57.375,\n",
      "    ],\n",
      "    type='SegDataPreProcessor')\n",
      "data_root = 'Glomeruli-dataset'\n",
      "dataset_type = 'StanfordBackgroundDataset'\n",
      "default_hooks = dict(\n",
      "    checkpoint=dict(\n",
      "        by_epoch=False,\n",
      "        filename_tmpl='CE{}.pth',\n",
      "        interval=400,\n",
      "        save_best='mIoU',\n",
      "        type='CheckpointHook'))\n",
      "default_scope = 'mmseg'\n",
      "env_cfg = dict(\n",
      "    cudnn_benchmark=True,\n",
      "    dist_cfg=dict(backend='nccl'),\n",
      "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
      "img_ratios = [\n",
      "    0.5,\n",
      "    0.75,\n",
      "    1.0,\n",
      "    1.25,\n",
      "    1.5,\n",
      "    1.75,\n",
      "]\n",
      "load_from = 'pspnet_r50-d8_512x1024_40k_cityscapes_20200605_003338-2966598c.pth'\n",
      "log_level = 'INFO'\n",
      "log_processor = dict(by_epoch=False)\n",
      "metainfo = dict(\n",
      "    classes=(\n",
      "        'background',\n",
      "        'glomeruili',\n",
      "    ),\n",
      "    palette=[\n",
      "        [\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "        ],\n",
      "        [\n",
      "            255,\n",
      "            0,\n",
      "            0,\n",
      "        ],\n",
      "    ])\n",
      "model = dict(\n",
      "    auxiliary_head=dict(\n",
      "        align_corners=False,\n",
      "        channels=256,\n",
      "        concat_input=False,\n",
      "        dropout_ratio=0.1,\n",
      "        in_channels=1024,\n",
      "        in_index=2,\n",
      "        loss_decode=dict(\n",
      "            loss_weight=0.4, type='CrossEntropyLoss', use_sigmoid=False),\n",
      "        norm_cfg=dict(requires_grad=True, type='BN'),\n",
      "        num_classes=2,\n",
      "        num_convs=1,\n",
      "        type='FCNHead'),\n",
      "    backbone=dict(\n",
      "        contract_dilation=True,\n",
      "        depth=50,\n",
      "        dilations=(\n",
      "            1,\n",
      "            1,\n",
      "            2,\n",
      "            4,\n",
      "        ),\n",
      "        norm_cfg=dict(requires_grad=True, type='BN'),\n",
      "        norm_eval=False,\n",
      "        num_stages=4,\n",
      "        out_indices=(\n",
      "            0,\n",
      "            1,\n",
      "            2,\n",
      "            3,\n",
      "        ),\n",
      "        strides=(\n",
      "            1,\n",
      "            2,\n",
      "            1,\n",
      "            1,\n",
      "        ),\n",
      "        style='pytorch',\n",
      "        type='ResNetV1c'),\n",
      "    data_preprocessor=dict(\n",
      "        bgr_to_rgb=True,\n",
      "        mean=[\n",
      "            123.675,\n",
      "            116.28,\n",
      "            103.53,\n",
      "        ],\n",
      "        pad_val=0,\n",
      "        seg_pad_val=255,\n",
      "        size=(\n",
      "            256,\n",
      "            256,\n",
      "        ),\n",
      "        std=[\n",
      "            58.395,\n",
      "            57.12,\n",
      "            57.375,\n",
      "        ],\n",
      "        type='SegDataPreProcessor'),\n",
      "    decode_head=dict(\n",
      "        align_corners=False,\n",
      "        channels=512,\n",
      "        dropout_ratio=0.1,\n",
      "        in_channels=2048,\n",
      "        in_index=3,\n",
      "        loss_decode=dict(\n",
      "            loss_weight=1.0, type='CrossEntropyLoss', use_sigmoid=False),\n",
      "        norm_cfg=dict(requires_grad=True, type='BN'),\n",
      "        num_classes=2,\n",
      "        pool_scales=(\n",
      "            1,\n",
      "            2,\n",
      "            3,\n",
      "            6,\n",
      "        ),\n",
      "        type='PSPHead'),\n",
      "    pretrained=None,\n",
      "    test_cfg=dict(mode='whole'),\n",
      "    train_cfg=None,\n",
      "    type='EncoderDecoder')\n",
      "norm_cfg = dict(requires_grad=True, type='BN')\n",
      "optim_wrapper = dict(\n",
      "    clip_grad=None,\n",
      "    optimizer=dict(lr=0.01, momentum=0.9, type='SGD', weight_decay=0.0005),\n",
      "    type='OptimWrapper')\n",
      "optimizer = dict(lr=0.01, momentum=0.9, type='SGD', weight_decay=0.0005)\n",
      "param_scheduler = [\n",
      "    dict(\n",
      "        begin=0,\n",
      "        by_epoch=False,\n",
      "        end=40000,\n",
      "        eta_min=0.0001,\n",
      "        power=0.9,\n",
      "        type='PolyLR'),\n",
      "]\n",
      "randomness = dict(seed=0)\n",
      "resume = False\n",
      "test_cfg = dict(type='TestLoop')\n",
      "test_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    dataset=dict(\n",
      "        ann_file='splits/val.txt',\n",
      "        data_prefix=dict(img_path='images', seg_map_path='masks'),\n",
      "        data_root='Glomeruli-dataset',\n",
      "        metainfo=dict(\n",
      "            classes=(\n",
      "                'background',\n",
      "                'glomeruili',\n",
      "            ),\n",
      "            palette=[\n",
      "                [\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                ],\n",
      "                [\n",
      "                    255,\n",
      "                    0,\n",
      "                    0,\n",
      "                ],\n",
      "            ]),\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(keep_ratio=True, scale=(\n",
      "                320,\n",
      "                240,\n",
      "            ), type='Resize'),\n",
      "            dict(type='LoadAnnotations'),\n",
      "            dict(type='PackSegInputs'),\n",
      "        ],\n",
      "        type='StanfordBackgroundDataset'),\n",
      "    num_workers=4,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "test_evaluator = dict(\n",
      "    iou_metrics=[\n",
      "        'mIoU',\n",
      "    ], type='IoUMetric')\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(keep_ratio=True, scale=(\n",
      "        320,\n",
      "        240,\n",
      "    ), type='Resize'),\n",
      "    dict(type='LoadAnnotations'),\n",
      "    dict(type='PackSegInputs'),\n",
      "]\n",
      "train_cfg = dict(max_iters=24000, type='IterBasedTrainLoop', val_interval=400)\n",
      "train_dataloader = dict(\n",
      "    batch_size=8,\n",
      "    dataset=dict(\n",
      "        ann_file='splits/train.txt',\n",
      "        data_prefix=dict(img_path='images', seg_map_path='masks'),\n",
      "        data_root='Glomeruli-dataset',\n",
      "        metainfo=dict(\n",
      "            classes=(\n",
      "                'background',\n",
      "                'glomeruili',\n",
      "            ),\n",
      "            palette=[\n",
      "                [\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                ],\n",
      "                [\n",
      "                    255,\n",
      "                    0,\n",
      "                    0,\n",
      "                ],\n",
      "            ]),\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(type='LoadAnnotations'),\n",
      "            dict(\n",
      "                keep_ratio=True,\n",
      "                ratio_range=(\n",
      "                    0.5,\n",
      "                    2.0,\n",
      "                ),\n",
      "                scale=(\n",
      "                    320,\n",
      "                    240,\n",
      "                ),\n",
      "                type='RandomResize'),\n",
      "            dict(\n",
      "                cat_max_ratio=0.75, crop_size=(\n",
      "                    256,\n",
      "                    256,\n",
      "                ), type='RandomCrop'),\n",
      "            dict(prob=0.5, type='RandomFlip'),\n",
      "            dict(type='PackSegInputs'),\n",
      "        ],\n",
      "        type='StanfordBackgroundDataset'),\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=True, type='InfiniteSampler'))\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations'),\n",
      "    dict(\n",
      "        keep_ratio=True,\n",
      "        ratio_range=(\n",
      "            0.5,\n",
      "            2.0,\n",
      "        ),\n",
      "        scale=(\n",
      "            320,\n",
      "            240,\n",
      "        ),\n",
      "        type='RandomResize'),\n",
      "    dict(cat_max_ratio=0.75, crop_size=(\n",
      "        256,\n",
      "        256,\n",
      "    ), type='RandomCrop'),\n",
      "    dict(prob=0.5, type='RandomFlip'),\n",
      "    dict(type='PackSegInputs'),\n",
      "]\n",
      "tta_model = dict(type='SegTTAModel')\n",
      "tta_pipeline = [\n",
      "    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "    dict(\n",
      "        transforms=[\n",
      "            [\n",
      "                dict(keep_ratio=True, scale_factor=0.5, type='Resize'),\n",
      "                dict(keep_ratio=True, scale_factor=0.75, type='Resize'),\n",
      "                dict(keep_ratio=True, scale_factor=1.0, type='Resize'),\n",
      "                dict(keep_ratio=True, scale_factor=1.25, type='Resize'),\n",
      "                dict(keep_ratio=True, scale_factor=1.5, type='Resize'),\n",
      "                dict(keep_ratio=True, scale_factor=1.75, type='Resize'),\n",
      "            ],\n",
      "            [\n",
      "                dict(direction='horizontal', prob=0.0, type='RandomFlip'),\n",
      "                dict(direction='horizontal', prob=1.0, type='RandomFlip'),\n",
      "            ],\n",
      "            [\n",
      "                dict(type='LoadAnnotations'),\n",
      "            ],\n",
      "            [\n",
      "                dict(type='PackSegInputs'),\n",
      "            ],\n",
      "        ],\n",
      "        type='TestTimeAug'),\n",
      "]\n",
      "val_cfg = dict(type='ValLoop')\n",
      "val_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    dataset=dict(\n",
      "        ann_file='splits/val.txt',\n",
      "        data_prefix=dict(img_path='images', seg_map_path='masks'),\n",
      "        data_root='Glomeruli-dataset',\n",
      "        metainfo=dict(\n",
      "            classes=(\n",
      "                'background',\n",
      "                'glomeruili',\n",
      "            ),\n",
      "            palette=[\n",
      "                [\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                ],\n",
      "                [\n",
      "                    255,\n",
      "                    0,\n",
      "                    0,\n",
      "                ],\n",
      "            ]),\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(keep_ratio=True, scale=(\n",
      "                320,\n",
      "                240,\n",
      "            ), type='Resize'),\n",
      "            dict(type='LoadAnnotations'),\n",
      "            dict(type='PackSegInputs'),\n",
      "        ],\n",
      "        type='StanfordBackgroundDataset'),\n",
      "    num_workers=4,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "val_evaluator = dict(\n",
      "    dataset_meta=dict(\n",
      "        classes=(\n",
      "            'background',\n",
      "            'glomeruili',\n",
      "        ),\n",
      "        palette=[\n",
      "            [\n",
      "                0,\n",
      "                0,\n",
      "                0,\n",
      "            ],\n",
      "            [\n",
      "                255,\n",
      "                0,\n",
      "                0,\n",
      "            ],\n",
      "        ]),\n",
      "    format_only=False,\n",
      "    iou_metrics=[\n",
      "        'mIoU',\n",
      "        'mDice',\n",
      "        'mFscore',\n",
      "    ],\n",
      "    output_dir='val_metrics',\n",
      "    type='IoUMetric')\n",
      "vis_backends = [\n",
      "    dict(type='LocalVisBackend'),\n",
      "]\n",
      "visualizer = dict(\n",
      "    name='visualizer',\n",
      "    type='SegLocalVisualizer',\n",
      "    vis_backends=[\n",
      "        dict(type='LocalVisBackend'),\n",
      "    ])\n",
      "work_dir = './work_dirs/tutorial'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(cfg.pretty_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 保存config配置文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.dump('new_cfg11.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWuH14LYF2gQ"
   },
   "source": [
    "## 准备训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jYKoSfdMF12B",
    "outputId": "422219ca-d7a5-4890-f09f-88c959942e64",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/29 19:35:39 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.7.10 (default, Jun  4 2021, 14:48:32) [GCC 7.5.0]\n",
      "    CUDA available: True\n",
      "    MUSA available: False\n",
      "    numpy_random_seed: 0\n",
      "    GPU 0: NVIDIA GeForce RTX 3060\n",
      "    CUDA_HOME: /usr/local/cuda\n",
      "    NVCC: Cuda compilation tools, release 11.2, V11.2.152\n",
      "    GCC: gcc (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0\n",
      "    PyTorch: 1.10.1+cu113\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 7.3\n",
      "  - C++ Version: 201402\n",
      "  - Intel(R) Math Kernel Library Version 2020.0.0 Product Build 20191122 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.2.3 (Git Hash 7336ca9f055cf1bfa13efb658fe15dc9b41f0740)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.3\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86\n",
      "  - CuDNN 8.2\n",
      "  - Magma 2.5.2\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.2.0, CXX_COMPILER=/opt/rh/devtoolset-7/root/usr/bin/c++, CXX_FLAGS= -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.10.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, \n",
      "\n",
      "    TorchVision: 0.11.2+cu113\n",
      "    OpenCV: 4.5.4\n",
      "    MMEngine: 0.10.7\n",
      "\n",
      "Runtime environment:\n",
      "    cudnn_benchmark: True\n",
      "    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: 0\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "04/29 19:35:39 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Config:\n",
      "crop_size = (\n",
      "    256,\n",
      "    256,\n",
      ")\n",
      "data_preprocessor = dict(\n",
      "    bgr_to_rgb=True,\n",
      "    mean=[\n",
      "        123.675,\n",
      "        116.28,\n",
      "        103.53,\n",
      "    ],\n",
      "    pad_val=0,\n",
      "    seg_pad_val=255,\n",
      "    size=(\n",
      "        512,\n",
      "        1024,\n",
      "    ),\n",
      "    std=[\n",
      "        58.395,\n",
      "        57.12,\n",
      "        57.375,\n",
      "    ],\n",
      "    type='SegDataPreProcessor')\n",
      "data_root = 'Glomeruli-dataset'\n",
      "dataset_type = 'StanfordBackgroundDataset'\n",
      "default_hooks = dict(\n",
      "    checkpoint=dict(\n",
      "        by_epoch=False,\n",
      "        filename_tmpl='CE{}.pth',\n",
      "        interval=400,\n",
      "        save_best='mIoU',\n",
      "        type='CheckpointHook'))\n",
      "default_scope = 'mmseg'\n",
      "env_cfg = dict(\n",
      "    cudnn_benchmark=True,\n",
      "    dist_cfg=dict(backend='nccl'),\n",
      "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
      "img_ratios = [\n",
      "    0.5,\n",
      "    0.75,\n",
      "    1.0,\n",
      "    1.25,\n",
      "    1.5,\n",
      "    1.75,\n",
      "]\n",
      "load_from = 'pspnet_r50-d8_512x1024_40k_cityscapes_20200605_003338-2966598c.pth'\n",
      "log_level = 'INFO'\n",
      "log_processor = dict(by_epoch=False)\n",
      "metainfo = dict(\n",
      "    classes=(\n",
      "        'background',\n",
      "        'glomeruili',\n",
      "    ),\n",
      "    palette=[\n",
      "        [\n",
      "            0,\n",
      "            0,\n",
      "            0,\n",
      "        ],\n",
      "        [\n",
      "            255,\n",
      "            0,\n",
      "            0,\n",
      "        ],\n",
      "    ])\n",
      "model = dict(\n",
      "    auxiliary_head=dict(\n",
      "        align_corners=False,\n",
      "        channels=256,\n",
      "        concat_input=False,\n",
      "        dropout_ratio=0.1,\n",
      "        in_channels=1024,\n",
      "        in_index=2,\n",
      "        loss_decode=dict(\n",
      "            loss_weight=0.4, type='CrossEntropyLoss', use_sigmoid=False),\n",
      "        norm_cfg=dict(requires_grad=True, type='BN'),\n",
      "        num_classes=2,\n",
      "        num_convs=1,\n",
      "        type='FCNHead'),\n",
      "    backbone=dict(\n",
      "        contract_dilation=True,\n",
      "        depth=50,\n",
      "        dilations=(\n",
      "            1,\n",
      "            1,\n",
      "            2,\n",
      "            4,\n",
      "        ),\n",
      "        norm_cfg=dict(requires_grad=True, type='BN'),\n",
      "        norm_eval=False,\n",
      "        num_stages=4,\n",
      "        out_indices=(\n",
      "            0,\n",
      "            1,\n",
      "            2,\n",
      "            3,\n",
      "        ),\n",
      "        strides=(\n",
      "            1,\n",
      "            2,\n",
      "            1,\n",
      "            1,\n",
      "        ),\n",
      "        style='pytorch',\n",
      "        type='ResNetV1c'),\n",
      "    data_preprocessor=dict(\n",
      "        bgr_to_rgb=True,\n",
      "        mean=[\n",
      "            123.675,\n",
      "            116.28,\n",
      "            103.53,\n",
      "        ],\n",
      "        pad_val=0,\n",
      "        seg_pad_val=255,\n",
      "        size=(\n",
      "            256,\n",
      "            256,\n",
      "        ),\n",
      "        std=[\n",
      "            58.395,\n",
      "            57.12,\n",
      "            57.375,\n",
      "        ],\n",
      "        type='SegDataPreProcessor'),\n",
      "    decode_head=dict(\n",
      "        align_corners=False,\n",
      "        channels=512,\n",
      "        dropout_ratio=0.1,\n",
      "        in_channels=2048,\n",
      "        in_index=3,\n",
      "        loss_decode=dict(\n",
      "            loss_weight=1.0, type='CrossEntropyLoss', use_sigmoid=False),\n",
      "        norm_cfg=dict(requires_grad=True, type='BN'),\n",
      "        num_classes=2,\n",
      "        pool_scales=(\n",
      "            1,\n",
      "            2,\n",
      "            3,\n",
      "            6,\n",
      "        ),\n",
      "        type='PSPHead'),\n",
      "    pretrained=None,\n",
      "    test_cfg=dict(mode='whole'),\n",
      "    train_cfg=None,\n",
      "    type='EncoderDecoder')\n",
      "norm_cfg = dict(requires_grad=True, type='BN')\n",
      "optim_wrapper = dict(\n",
      "    clip_grad=None,\n",
      "    optimizer=dict(lr=0.01, momentum=0.9, type='SGD', weight_decay=0.0005),\n",
      "    type='OptimWrapper')\n",
      "optimizer = dict(lr=0.01, momentum=0.9, type='SGD', weight_decay=0.0005)\n",
      "param_scheduler = [\n",
      "    dict(\n",
      "        begin=0,\n",
      "        by_epoch=False,\n",
      "        end=40000,\n",
      "        eta_min=0.0001,\n",
      "        power=0.9,\n",
      "        type='PolyLR'),\n",
      "]\n",
      "randomness = dict(seed=0)\n",
      "resume = False\n",
      "test_cfg = dict(type='TestLoop')\n",
      "test_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    dataset=dict(\n",
      "        ann_file='splits/val.txt',\n",
      "        data_prefix=dict(img_path='images', seg_map_path='masks'),\n",
      "        data_root='Glomeruli-dataset',\n",
      "        metainfo=dict(\n",
      "            classes=(\n",
      "                'background',\n",
      "                'glomeruili',\n",
      "            ),\n",
      "            palette=[\n",
      "                [\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                ],\n",
      "                [\n",
      "                    255,\n",
      "                    0,\n",
      "                    0,\n",
      "                ],\n",
      "            ]),\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(keep_ratio=True, scale=(\n",
      "                320,\n",
      "                240,\n",
      "            ), type='Resize'),\n",
      "            dict(type='LoadAnnotations'),\n",
      "            dict(type='PackSegInputs'),\n",
      "        ],\n",
      "        type='StanfordBackgroundDataset'),\n",
      "    num_workers=4,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "test_evaluator = dict(\n",
      "    iou_metrics=[\n",
      "        'mIoU',\n",
      "    ], type='IoUMetric')\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(keep_ratio=True, scale=(\n",
      "        320,\n",
      "        240,\n",
      "    ), type='Resize'),\n",
      "    dict(type='LoadAnnotations'),\n",
      "    dict(type='PackSegInputs'),\n",
      "]\n",
      "train_cfg = dict(max_iters=24000, type='IterBasedTrainLoop', val_interval=400)\n",
      "train_dataloader = dict(\n",
      "    batch_size=8,\n",
      "    dataset=dict(\n",
      "        ann_file='splits/train.txt',\n",
      "        data_prefix=dict(img_path='images', seg_map_path='masks'),\n",
      "        data_root='Glomeruli-dataset',\n",
      "        metainfo=dict(\n",
      "            classes=(\n",
      "                'background',\n",
      "                'glomeruili',\n",
      "            ),\n",
      "            palette=[\n",
      "                [\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                ],\n",
      "                [\n",
      "                    255,\n",
      "                    0,\n",
      "                    0,\n",
      "                ],\n",
      "            ]),\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(type='LoadAnnotations'),\n",
      "            dict(\n",
      "                keep_ratio=True,\n",
      "                ratio_range=(\n",
      "                    0.5,\n",
      "                    2.0,\n",
      "                ),\n",
      "                scale=(\n",
      "                    320,\n",
      "                    240,\n",
      "                ),\n",
      "                type='RandomResize'),\n",
      "            dict(\n",
      "                cat_max_ratio=0.75, crop_size=(\n",
      "                    256,\n",
      "                    256,\n",
      "                ), type='RandomCrop'),\n",
      "            dict(prob=0.5, type='RandomFlip'),\n",
      "            dict(type='PackSegInputs'),\n",
      "        ],\n",
      "        type='StanfordBackgroundDataset'),\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=True, type='InfiniteSampler'))\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations'),\n",
      "    dict(\n",
      "        keep_ratio=True,\n",
      "        ratio_range=(\n",
      "            0.5,\n",
      "            2.0,\n",
      "        ),\n",
      "        scale=(\n",
      "            320,\n",
      "            240,\n",
      "        ),\n",
      "        type='RandomResize'),\n",
      "    dict(cat_max_ratio=0.75, crop_size=(\n",
      "        256,\n",
      "        256,\n",
      "    ), type='RandomCrop'),\n",
      "    dict(prob=0.5, type='RandomFlip'),\n",
      "    dict(type='PackSegInputs'),\n",
      "]\n",
      "tta_model = dict(type='SegTTAModel')\n",
      "tta_pipeline = [\n",
      "    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "    dict(\n",
      "        transforms=[\n",
      "            [\n",
      "                dict(keep_ratio=True, scale_factor=0.5, type='Resize'),\n",
      "                dict(keep_ratio=True, scale_factor=0.75, type='Resize'),\n",
      "                dict(keep_ratio=True, scale_factor=1.0, type='Resize'),\n",
      "                dict(keep_ratio=True, scale_factor=1.25, type='Resize'),\n",
      "                dict(keep_ratio=True, scale_factor=1.5, type='Resize'),\n",
      "                dict(keep_ratio=True, scale_factor=1.75, type='Resize'),\n",
      "            ],\n",
      "            [\n",
      "                dict(direction='horizontal', prob=0.0, type='RandomFlip'),\n",
      "                dict(direction='horizontal', prob=1.0, type='RandomFlip'),\n",
      "            ],\n",
      "            [\n",
      "                dict(type='LoadAnnotations'),\n",
      "            ],\n",
      "            [\n",
      "                dict(type='PackSegInputs'),\n",
      "            ],\n",
      "        ],\n",
      "        type='TestTimeAug'),\n",
      "]\n",
      "val_cfg = dict(type='ValLoop')\n",
      "val_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    dataset=dict(\n",
      "        ann_file='splits/val.txt',\n",
      "        data_prefix=dict(img_path='images', seg_map_path='masks'),\n",
      "        data_root='Glomeruli-dataset',\n",
      "        metainfo=dict(\n",
      "            classes=(\n",
      "                'background',\n",
      "                'glomeruili',\n",
      "            ),\n",
      "            palette=[\n",
      "                [\n",
      "                    0,\n",
      "                    0,\n",
      "                    0,\n",
      "                ],\n",
      "                [\n",
      "                    255,\n",
      "                    0,\n",
      "                    0,\n",
      "                ],\n",
      "            ]),\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(keep_ratio=True, scale=(\n",
      "                320,\n",
      "                240,\n",
      "            ), type='Resize'),\n",
      "            dict(type='LoadAnnotations'),\n",
      "            dict(type='PackSegInputs'),\n",
      "        ],\n",
      "        type='StanfordBackgroundDataset'),\n",
      "    num_workers=4,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "val_evaluator = dict(\n",
      "    dataset_meta=dict(\n",
      "        classes=(\n",
      "            'background',\n",
      "            'glomeruili',\n",
      "        ),\n",
      "        palette=[\n",
      "            [\n",
      "                0,\n",
      "                0,\n",
      "                0,\n",
      "            ],\n",
      "            [\n",
      "                255,\n",
      "                0,\n",
      "                0,\n",
      "            ],\n",
      "        ]),\n",
      "    format_only=False,\n",
      "    iou_metrics=[\n",
      "        'mIoU',\n",
      "        'mDice',\n",
      "        'mFscore',\n",
      "    ],\n",
      "    output_dir='val_metrics',\n",
      "    type='IoUMetric')\n",
      "vis_backends = [\n",
      "    dict(type='LocalVisBackend'),\n",
      "]\n",
      "visualizer = dict(\n",
      "    name='visualizer',\n",
      "    type='SegLocalVisualizer',\n",
      "    vis_backends=[\n",
      "        dict(type='LocalVisBackend'),\n",
      "    ])\n",
      "work_dir = './work_dirs/tutorial'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/environment/miniconda3/lib/python3.7/site-packages/mmengine/utils/manager.py:114: UserWarning: <class 'mmseg.visualization.local_visualizer.SegLocalVisualizer'> instance named of visualizer has been created, the method `get_instance` should not accept any other arguments\n",
      "  f'{cls} instance named of {name} has been created, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/29 19:35:40 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "04/29 19:35:40 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DistSamplerSeedHook                \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n"
     ]
    }
   ],
   "source": [
    "from mmengine.runner import Runner\n",
    "from mmseg.utils import register_all_modules\n",
    "\n",
    "# register all modules in mmseg into the registries\n",
    "# do not init the default scope here because it will be init in the runner\n",
    "register_all_modules(init_default_scope=False)\n",
    "runner = Runner.from_cfg(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练\n",
    "\n",
    "如果遇到报错`CUDA out of memeory`，重启实例或使用显存更高的实例即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/29 19:35:41 - mmengine - \u001B[5m\u001B[4m\u001B[33mWARNING\u001B[0m - The prefix is not set in metric class IoUMetric.\n",
      "Loads checkpoint by local backend from path: pspnet_r50-d8_512x1024_40k_cityscapes_20200605_003338-2966598c.pth\n",
      "The model and loaded state dict do not match exactly\n",
      "\n",
      "size mismatch for decode_head.conv_seg.weight: copying a param with shape torch.Size([19, 512, 1, 1]) from checkpoint, the shape in current model is torch.Size([2, 512, 1, 1]).\n",
      "size mismatch for decode_head.conv_seg.bias: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([2]).\n",
      "size mismatch for auxiliary_head.conv_seg.weight: copying a param with shape torch.Size([19, 256, 1, 1]) from checkpoint, the shape in current model is torch.Size([2, 256, 1, 1]).\n",
      "size mismatch for auxiliary_head.conv_seg.bias: copying a param with shape torch.Size([19]) from checkpoint, the shape in current model is torch.Size([2]).\n",
      "04/29 19:35:42 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Load checkpoint from pspnet_r50-d8_512x1024_40k_cityscapes_20200605_003338-2966598c.pth\n",
      "04/29 19:35:42 - mmengine - \u001B[5m\u001B[4m\u001B[33mWARNING\u001B[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "04/29 19:35:42 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Checkpoints will be saved to /home/featurize/work/MMSegmentation_Tutorials-main/20230215/【D1】Kaggle代码实战-肾小球切片语义分割/work_dirs/tutorial.\n",
      "04/29 19:35:45 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [   10/24000]  lr: 9.9980e-03  eta: 2:10:45  time: 0.3270  data_time: 0.0108  memory: 4530  loss: 0.3718  decode.loss_ce: 0.2301  decode.acc_seg: 97.1767  aux.loss_ce: 0.1417  aux.acc_seg: 97.1767\n",
      "04/29 19:35:48 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [   20/24000]  lr: 9.9958e-03  eta: 2:10:08  time: 0.3242  data_time: 0.0111  memory: 4530  loss: 0.1290  decode.loss_ce: 0.0816  decode.acc_seg: 97.8641  aux.loss_ce: 0.0474  aux.acc_seg: 97.8641\n",
      "04/29 19:35:52 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [   30/24000]  lr: 9.9935e-03  eta: 2:09:55  time: 0.3244  data_time: 0.0111  memory: 4530  loss: 0.0878  decode.loss_ce: 0.0605  decode.acc_seg: 94.8752  aux.loss_ce: 0.0274  aux.acc_seg: 94.8752\n",
      "04/29 19:35:55 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [   40/24000]  lr: 9.9913e-03  eta: 2:09:51  time: 0.3251  data_time: 0.0110  memory: 4530  loss: 0.1013  decode.loss_ce: 0.0718  decode.acc_seg: 99.6067  aux.loss_ce: 0.0295  aux.acc_seg: 99.6067\n",
      "04/29 19:35:58 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [   50/24000]  lr: 9.9891e-03  eta: 2:09:54  time: 0.3265  data_time: 0.0113  memory: 4530  loss: 0.0821  decode.loss_ce: 0.0565  decode.acc_seg: 99.7613  aux.loss_ce: 0.0256  aux.acc_seg: 99.7613\n",
      "04/29 19:36:01 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [   60/24000]  lr: 9.9869e-03  eta: 2:10:07  time: 0.3294  data_time: 0.0121  memory: 4530  loss: 0.1492  decode.loss_ce: 0.1057  decode.acc_seg: 97.9067  aux.loss_ce: 0.0435  aux.acc_seg: 97.9067\n",
      "04/29 19:36:05 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [   70/24000]  lr: 9.9846e-03  eta: 2:10:05  time: 0.3266  data_time: 0.0115  memory: 4530  loss: 0.0756  decode.loss_ce: 0.0549  decode.acc_seg: 100.0000  aux.loss_ce: 0.0207  aux.acc_seg: 100.0000\n",
      "04/29 19:36:08 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [   80/24000]  lr: 9.9824e-03  eta: 2:10:03  time: 0.3267  data_time: 0.0114  memory: 4530  loss: 0.0531  decode.loss_ce: 0.0362  decode.acc_seg: 98.9285  aux.loss_ce: 0.0169  aux.acc_seg: 98.9285\n",
      "04/29 19:36:11 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [   90/24000]  lr: 9.9802e-03  eta: 2:10:01  time: 0.3268  data_time: 0.0114  memory: 4530  loss: 0.0890  decode.loss_ce: 0.0642  decode.acc_seg: 99.4359  aux.loss_ce: 0.0248  aux.acc_seg: 99.4359\n",
      "04/29 19:36:14 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  100/24000]  lr: 9.9779e-03  eta: 2:10:10  time: 0.3315  data_time: 0.0130  memory: 4530  loss: 0.0679  decode.loss_ce: 0.0454  decode.acc_seg: 97.8993  aux.loss_ce: 0.0225  aux.acc_seg: 97.8993\n",
      "04/29 19:36:18 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  110/24000]  lr: 9.9757e-03  eta: 2:10:11  time: 0.3285  data_time: 0.0120  memory: 4530  loss: 0.0707  decode.loss_ce: 0.0466  decode.acc_seg: 97.4600  aux.loss_ce: 0.0241  aux.acc_seg: 97.4600\n",
      "04/29 19:36:21 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  120/24000]  lr: 9.9735e-03  eta: 2:10:09  time: 0.3275  data_time: 0.0114  memory: 4530  loss: 0.0811  decode.loss_ce: 0.0554  decode.acc_seg: 95.3150  aux.loss_ce: 0.0257  aux.acc_seg: 95.3150\n",
      "04/29 19:36:24 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  130/24000]  lr: 9.9713e-03  eta: 2:10:10  time: 0.3297  data_time: 0.0129  memory: 4530  loss: 0.0716  decode.loss_ce: 0.0518  decode.acc_seg: 97.1695  aux.loss_ce: 0.0198  aux.acc_seg: 97.1695\n",
      "04/29 19:36:28 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  140/24000]  lr: 9.9690e-03  eta: 2:10:09  time: 0.3283  data_time: 0.0117  memory: 4530  loss: 0.0711  decode.loss_ce: 0.0497  decode.acc_seg: 98.3498  aux.loss_ce: 0.0214  aux.acc_seg: 97.1830\n",
      "04/29 19:36:31 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  150/24000]  lr: 9.9668e-03  eta: 2:10:07  time: 0.3280  data_time: 0.0117  memory: 4530  loss: 0.0785  decode.loss_ce: 0.0554  decode.acc_seg: 98.7255  aux.loss_ce: 0.0231  aux.acc_seg: 97.7285\n",
      "04/29 19:36:34 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  160/24000]  lr: 9.9646e-03  eta: 2:10:09  time: 0.3308  data_time: 0.0123  memory: 4530  loss: 0.0712  decode.loss_ce: 0.0492  decode.acc_seg: 98.8767  aux.loss_ce: 0.0220  aux.acc_seg: 98.9811\n",
      "04/29 19:36:37 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  170/24000]  lr: 9.9623e-03  eta: 2:10:09  time: 0.3302  data_time: 0.0120  memory: 4530  loss: 0.0735  decode.loss_ce: 0.0517  decode.acc_seg: 99.3374  aux.loss_ce: 0.0218  aux.acc_seg: 99.3374\n",
      "04/29 19:36:41 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  180/24000]  lr: 9.9601e-03  eta: 2:10:05  time: 0.3270  data_time: 0.0111  memory: 4530  loss: 0.0798  decode.loss_ce: 0.0556  decode.acc_seg: 93.8620  aux.loss_ce: 0.0243  aux.acc_seg: 93.0362\n",
      "04/29 19:36:44 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  190/24000]  lr: 9.9579e-03  eta: 2:10:01  time: 0.3273  data_time: 0.0112  memory: 4530  loss: 0.0569  decode.loss_ce: 0.0390  decode.acc_seg: 99.3857  aux.loss_ce: 0.0180  aux.acc_seg: 99.3852\n",
      "04/29 19:36:47 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  200/24000]  lr: 9.9557e-03  eta: 2:09:58  time: 0.3275  data_time: 0.0114  memory: 4530  loss: 0.0506  decode.loss_ce: 0.0347  decode.acc_seg: 98.6217  aux.loss_ce: 0.0159  aux.acc_seg: 98.6927\n",
      "04/29 19:36:51 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  210/24000]  lr: 9.9534e-03  eta: 2:09:54  time: 0.3274  data_time: 0.0115  memory: 4530  loss: 0.0682  decode.loss_ce: 0.0497  decode.acc_seg: 90.9718  aux.loss_ce: 0.0186  aux.acc_seg: 90.9718\n",
      "04/29 19:36:54 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  220/24000]  lr: 9.9512e-03  eta: 2:09:51  time: 0.3278  data_time: 0.0115  memory: 4530  loss: 0.0628  decode.loss_ce: 0.0446  decode.acc_seg: 97.4640  aux.loss_ce: 0.0182  aux.acc_seg: 93.0177\n",
      "04/29 19:36:57 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  230/24000]  lr: 9.9490e-03  eta: 2:09:48  time: 0.3279  data_time: 0.0115  memory: 4530  loss: 0.0535  decode.loss_ce: 0.0370  decode.acc_seg: 99.9594  aux.loss_ce: 0.0165  aux.acc_seg: 100.0000\n",
      "04/29 19:37:00 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  240/24000]  lr: 9.9467e-03  eta: 2:09:44  time: 0.3277  data_time: 0.0114  memory: 4530  loss: 0.0411  decode.loss_ce: 0.0277  decode.acc_seg: 98.3294  aux.loss_ce: 0.0134  aux.acc_seg: 96.6954\n",
      "04/29 19:37:04 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  250/24000]  lr: 9.9445e-03  eta: 2:09:41  time: 0.3275  data_time: 0.0113  memory: 4530  loss: 0.0451  decode.loss_ce: 0.0310  decode.acc_seg: 99.7409  aux.loss_ce: 0.0140  aux.acc_seg: 99.1965\n",
      "04/29 19:37:06 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Exp name: pspnet_r50-d8_4xb2-40k_cityscapes-512x1024_20250429_193538\n",
      "04/29 19:37:07 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  260/24000]  lr: 9.9423e-03  eta: 2:09:39  time: 0.3291  data_time: 0.0120  memory: 4530  loss: 0.0619  decode.loss_ce: 0.0418  decode.acc_seg: 99.5621  aux.loss_ce: 0.0201  aux.acc_seg: 99.5577\n",
      "04/29 19:37:10 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  270/24000]  lr: 9.9401e-03  eta: 2:09:37  time: 0.3290  data_time: 0.0111  memory: 4530  loss: 0.0447  decode.loss_ce: 0.0318  decode.acc_seg: 98.3612  aux.loss_ce: 0.0128  aux.acc_seg: 98.3612\n",
      "04/29 19:37:14 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  280/24000]  lr: 9.9378e-03  eta: 2:09:35  time: 0.3290  data_time: 0.0111  memory: 4530  loss: 0.0610  decode.loss_ce: 0.0429  decode.acc_seg: 97.6202  aux.loss_ce: 0.0181  aux.acc_seg: 96.2205\n",
      "04/29 19:37:17 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  290/24000]  lr: 9.9356e-03  eta: 2:09:31  time: 0.3276  data_time: 0.0113  memory: 4530  loss: 0.0758  decode.loss_ce: 0.0550  decode.acc_seg: 94.8181  aux.loss_ce: 0.0209  aux.acc_seg: 98.6672\n",
      "04/29 19:37:20 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  300/24000]  lr: 9.9334e-03  eta: 2:09:30  time: 0.3297  data_time: 0.0132  memory: 4530  loss: 0.0605  decode.loss_ce: 0.0423  decode.acc_seg: 99.0622  aux.loss_ce: 0.0181  aux.acc_seg: 99.0622\n",
      "04/29 19:37:23 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  310/24000]  lr: 9.9311e-03  eta: 2:09:25  time: 0.3258  data_time: 0.0113  memory: 4530  loss: 0.0441  decode.loss_ce: 0.0305  decode.acc_seg: 99.0513  aux.loss_ce: 0.0136  aux.acc_seg: 99.4953\n",
      "04/29 19:37:27 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  320/24000]  lr: 9.9289e-03  eta: 2:09:20  time: 0.3254  data_time: 0.0110  memory: 4530  loss: 0.0357  decode.loss_ce: 0.0243  decode.acc_seg: 100.0000  aux.loss_ce: 0.0113  aux.acc_seg: 100.0000\n",
      "04/29 19:37:30 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  330/24000]  lr: 9.9267e-03  eta: 2:09:16  time: 0.3266  data_time: 0.0117  memory: 4530  loss: 0.0609  decode.loss_ce: 0.0417  decode.acc_seg: 98.8686  aux.loss_ce: 0.0191  aux.acc_seg: 97.8897\n",
      "04/29 19:37:33 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  340/24000]  lr: 9.9245e-03  eta: 2:09:11  time: 0.3258  data_time: 0.0115  memory: 4530  loss: 0.0470  decode.loss_ce: 0.0333  decode.acc_seg: 99.4850  aux.loss_ce: 0.0137  aux.acc_seg: 99.6720\n",
      "04/29 19:37:36 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  350/24000]  lr: 9.9222e-03  eta: 2:09:06  time: 0.3257  data_time: 0.0115  memory: 4530  loss: 0.0146  decode.loss_ce: 0.0096  decode.acc_seg: 100.0000  aux.loss_ce: 0.0050  aux.acc_seg: 100.0000\n",
      "04/29 19:37:40 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  360/24000]  lr: 9.9200e-03  eta: 2:09:02  time: 0.3257  data_time: 0.0114  memory: 4530  loss: 0.0486  decode.loss_ce: 0.0348  decode.acc_seg: 98.4030  aux.loss_ce: 0.0138  aux.acc_seg: 99.7761\n",
      "04/29 19:37:43 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  370/24000]  lr: 9.9178e-03  eta: 2:08:57  time: 0.3253  data_time: 0.0112  memory: 4530  loss: 0.0230  decode.loss_ce: 0.0157  decode.acc_seg: 99.2696  aux.loss_ce: 0.0073  aux.acc_seg: 98.9356\n",
      "04/29 19:37:46 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  380/24000]  lr: 9.9155e-03  eta: 2:08:53  time: 0.3254  data_time: 0.0113  memory: 4530  loss: 0.0313  decode.loss_ce: 0.0219  decode.acc_seg: 98.1747  aux.loss_ce: 0.0094  aux.acc_seg: 98.5925\n",
      "04/29 19:37:49 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  390/24000]  lr: 9.9133e-03  eta: 2:08:48  time: 0.3256  data_time: 0.0112  memory: 4530  loss: 0.0376  decode.loss_ce: 0.0259  decode.acc_seg: 99.3164  aux.loss_ce: 0.0117  aux.acc_seg: 98.8982\n",
      "04/29 19:37:53 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  400/24000]  lr: 9.9111e-03  eta: 2:08:44  time: 0.3256  data_time: 0.0113  memory: 4530  loss: 0.0263  decode.loss_ce: 0.0175  decode.acc_seg: 99.1763  aux.loss_ce: 0.0088  aux.acc_seg: 99.0205\n",
      "04/29 19:37:53 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Saving checkpoint at 400 iterations\n",
      "04/29 19:37:56 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [ 10/515]    eta: 0:00:47  time: 0.0944  data_time: 0.0390  memory: 1564  \n",
      "04/29 19:37:56 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [ 20/515]    eta: 0:00:37  time: 0.0576  data_time: 0.0037  memory: 1564  \n",
      "04/29 19:37:57 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [ 30/515]    eta: 0:00:33  time: 0.0572  data_time: 0.0035  memory: 1564  \n",
      "04/29 19:37:58 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [ 40/515]    eta: 0:00:31  time: 0.0569  data_time: 0.0032  memory: 1564  \n",
      "04/29 19:37:58 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [ 50/515]    eta: 0:00:30  time: 0.0570  data_time: 0.0036  memory: 1564  \n",
      "04/29 19:37:59 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [ 60/515]    eta: 0:00:28  time: 0.0574  data_time: 0.0031  memory: 1564  \n",
      "04/29 19:37:59 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [ 70/515]    eta: 0:00:27  time: 0.0568  data_time: 0.0035  memory: 1564  \n",
      "04/29 19:38:00 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [ 80/515]    eta: 0:00:26  time: 0.0568  data_time: 0.0031  memory: 1564  \n",
      "04/29 19:38:00 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [ 90/515]    eta: 0:00:26  time: 0.0577  data_time: 0.0036  memory: 1564  \n",
      "04/29 19:38:01 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [100/515]    eta: 0:00:25  time: 0.0576  data_time: 0.0034  memory: 1564  \n",
      "04/29 19:38:02 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [110/515]    eta: 0:00:24  time: 0.0575  data_time: 0.0037  memory: 1564  \n",
      "04/29 19:38:02 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [120/515]    eta: 0:00:24  time: 0.0665  data_time: 0.0039  memory: 1564  \n",
      "04/29 19:38:03 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [130/515]    eta: 0:00:23  time: 0.0652  data_time: 0.0036  memory: 1564  \n",
      "04/29 19:38:04 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [140/515]    eta: 0:00:23  time: 0.0692  data_time: 0.0041  memory: 1564  \n",
      "04/29 19:38:04 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [150/515]    eta: 0:00:22  time: 0.0682  data_time: 0.0040  memory: 1564  \n",
      "04/29 19:38:05 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [160/515]    eta: 0:00:22  time: 0.0638  data_time: 0.0037  memory: 1564  \n",
      "04/29 19:38:06 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [170/515]    eta: 0:00:21  time: 0.0678  data_time: 0.0038  memory: 1564  \n",
      "04/29 19:38:06 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [180/515]    eta: 0:00:21  time: 0.0636  data_time: 0.0038  memory: 1564  \n",
      "04/29 19:38:07 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [190/515]    eta: 0:00:20  time: 0.0671  data_time: 0.0041  memory: 1564  \n",
      "04/29 19:38:08 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [200/515]    eta: 0:00:19  time: 0.0632  data_time: 0.0043  memory: 1564  \n",
      "04/29 19:38:08 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [210/515]    eta: 0:00:19  time: 0.0596  data_time: 0.0035  memory: 1564  \n",
      "04/29 19:38:09 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [220/515]    eta: 0:00:18  time: 0.0634  data_time: 0.0043  memory: 1564  \n",
      "04/29 19:38:09 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [230/515]    eta: 0:00:17  time: 0.0656  data_time: 0.0040  memory: 1564  \n",
      "04/29 19:38:10 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [240/515]    eta: 0:00:17  time: 0.0631  data_time: 0.0038  memory: 1564  \n",
      "04/29 19:38:11 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [250/515]    eta: 0:00:16  time: 0.0586  data_time: 0.0032  memory: 1564  \n",
      "04/29 19:38:11 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [260/515]    eta: 0:00:15  time: 0.0591  data_time: 0.0032  memory: 1564  \n",
      "04/29 19:38:12 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [270/515]    eta: 0:00:15  time: 0.0594  data_time: 0.0032  memory: 1564  \n",
      "04/29 19:38:12 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [280/515]    eta: 0:00:14  time: 0.0582  data_time: 0.0031  memory: 1564  \n",
      "04/29 19:38:13 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [290/515]    eta: 0:00:14  time: 0.0592  data_time: 0.0031  memory: 1564  \n",
      "04/29 19:38:14 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [300/515]    eta: 0:00:13  time: 0.0582  data_time: 0.0032  memory: 1564  \n",
      "04/29 19:38:14 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [310/515]    eta: 0:00:12  time: 0.0585  data_time: 0.0031  memory: 1564  \n",
      "04/29 19:38:15 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [320/515]    eta: 0:00:12  time: 0.0649  data_time: 0.0044  memory: 1564  \n",
      "04/29 19:38:15 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [330/515]    eta: 0:00:11  time: 0.0576  data_time: 0.0035  memory: 1564  \n",
      "04/29 19:38:16 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [340/515]    eta: 0:00:10  time: 0.0619  data_time: 0.0039  memory: 1564  \n",
      "04/29 19:38:17 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [350/515]    eta: 0:00:10  time: 0.0574  data_time: 0.0031  memory: 1564  \n",
      "04/29 19:38:17 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [360/515]    eta: 0:00:09  time: 0.0579  data_time: 0.0032  memory: 1564  \n",
      "04/29 19:38:18 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [370/515]    eta: 0:00:08  time: 0.0582  data_time: 0.0031  memory: 1564  \n",
      "04/29 19:38:18 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [380/515]    eta: 0:00:08  time: 0.0577  data_time: 0.0031  memory: 1564  \n",
      "04/29 19:38:19 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [390/515]    eta: 0:00:07  time: 0.0583  data_time: 0.0032  memory: 1564  \n",
      "04/29 19:38:19 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [400/515]    eta: 0:00:07  time: 0.0584  data_time: 0.0032  memory: 1564  \n",
      "04/29 19:38:20 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [410/515]    eta: 0:00:06  time: 0.0586  data_time: 0.0032  memory: 1564  \n",
      "04/29 19:38:21 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [420/515]    eta: 0:00:05  time: 0.0581  data_time: 0.0031  memory: 1564  \n",
      "04/29 19:38:21 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [430/515]    eta: 0:00:05  time: 0.0590  data_time: 0.0031  memory: 1564  \n",
      "04/29 19:38:22 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [440/515]    eta: 0:00:04  time: 0.0591  data_time: 0.0032  memory: 1564  \n",
      "04/29 19:38:22 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [450/515]    eta: 0:00:03  time: 0.0585  data_time: 0.0031  memory: 1564  \n",
      "04/29 19:38:23 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [460/515]    eta: 0:00:03  time: 0.0585  data_time: 0.0031  memory: 1564  \n",
      "04/29 19:38:24 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [470/515]    eta: 0:00:02  time: 0.0585  data_time: 0.0031  memory: 1564  \n",
      "04/29 19:38:24 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [480/515]    eta: 0:00:02  time: 0.0589  data_time: 0.0032  memory: 1564  \n",
      "04/29 19:38:25 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [490/515]    eta: 0:00:01  time: 0.0580  data_time: 0.0032  memory: 1564  \n",
      "04/29 19:38:25 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [500/515]    eta: 0:00:00  time: 0.0574  data_time: 0.0030  memory: 1564  \n",
      "04/29 19:38:26 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [510/515]    eta: 0:00:00  time: 0.0588  data_time: 0.0031  memory: 1564  \n",
      "04/29 19:38:26 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - per class results:\n",
      "04/29 19:38:26 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - \n",
      "+------------+-------+-------+-------+--------+-----------+--------+\n",
      "|   Class    |  IoU  |  Acc  |  Dice | Fscore | Precision | Recall |\n",
      "+------------+-------+-------+-------+--------+-----------+--------+\n",
      "| background | 99.34 | 99.66 | 99.67 | 99.67  |   99.68   | 99.66  |\n",
      "| glomeruili | 62.77 | 77.73 | 77.13 | 77.13  |   76.53   | 77.73  |\n",
      "+------------+-------+-------+-------+--------+-----------+--------+\n",
      "04/29 19:38:26 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(val) [515/515]    aAcc: 99.3400  mIoU: 81.0500  mAcc: 88.6900  mDice: 88.4000  mFscore: 88.4000  mPrecision: 88.1100  mRecall: 88.6900  data_time: 0.0041  time: 0.0608\n",
      "04/29 19:38:28 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - The best checkpoint with 81.0500 mIoU at 400 iter is saved to best_mIoU_CE400.pth.\n",
      "04/29 19:38:35 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  410/24000]  lr: 9.9088e-03  eta: 2:14:03  time: 0.8880  data_time: 0.5694  memory: 4530  loss: 0.0233  decode.loss_ce: 0.0151  decode.acc_seg: 99.3113  aux.loss_ce: 0.0083  aux.acc_seg: 99.3427\n",
      "04/29 19:38:38 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  420/24000]  lr: 9.9066e-03  eta: 2:13:53  time: 0.3279  data_time: 0.0127  memory: 4530  loss: 0.0538  decode.loss_ce: 0.0363  decode.acc_seg: 99.4203  aux.loss_ce: 0.0176  aux.acc_seg: 99.4817\n",
      "04/29 19:38:42 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  430/24000]  lr: 9.9044e-03  eta: 2:13:42  time: 0.3276  data_time: 0.0123  memory: 4530  loss: 0.0469  decode.loss_ce: 0.0311  decode.acc_seg: 98.4710  aux.loss_ce: 0.0157  aux.acc_seg: 99.1320\n",
      "04/29 19:38:45 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  440/24000]  lr: 9.9022e-03  eta: 2:13:31  time: 0.3270  data_time: 0.0118  memory: 4530  loss: 0.0290  decode.loss_ce: 0.0194  decode.acc_seg: 99.6834  aux.loss_ce: 0.0096  aux.acc_seg: 99.3423\n",
      "04/29 19:38:48 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  450/24000]  lr: 9.8999e-03  eta: 2:13:22  time: 0.3278  data_time: 0.0120  memory: 4530  loss: 0.0486  decode.loss_ce: 0.0326  decode.acc_seg: 99.7984  aux.loss_ce: 0.0160  aux.acc_seg: 99.9733\n",
      "04/29 19:38:52 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  460/24000]  lr: 9.8977e-03  eta: 2:13:13  time: 0.3288  data_time: 0.0125  memory: 4530  loss: 0.0474  decode.loss_ce: 0.0335  decode.acc_seg: 99.1013  aux.loss_ce: 0.0139  aux.acc_seg: 98.8262\n",
      "04/29 19:38:55 - mmengine - \u001B[4m\u001B[37mINFO\u001B[0m - Iter(train) [  470/24000]  lr: 9.8955e-03  eta: 2:13:04  time: 0.3282  data_time: 0.0121  memory: 4530  loss: 0.0266  decode.loss_ce: 0.0177  decode.acc_seg: 98.7492  aux.loss_ce: 0.0090  aux.acc_seg: 99.5707\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_20846/3630031848.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mrunner\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/environment/miniconda3/lib/python3.7/site-packages/mmengine/runner/runner.py\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1775\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_maybe_compile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'train_step'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1776\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1777\u001B[0;31m         \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain_loop\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1778\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcall_hook\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'after_run'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1779\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/environment/miniconda3/lib/python3.7/site-packages/mmengine/runner/loops.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    286\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrunner\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    287\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 288\u001B[0;31m             \u001B[0mdata_batch\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataloader_iterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    289\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun_iter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata_batch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    290\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/environment/miniconda3/lib/python3.7/site-packages/mmengine/runner/loops.py\u001B[0m in \u001B[0;36m__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    165\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__next__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m->\u001B[0m \u001B[0mSequence\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mdict\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    166\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 167\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_iterator\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    168\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mStopIteration\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    169\u001B[0m             print_log(\n",
      "\u001B[0;32m/environment/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    519\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_sampler_iter\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    520\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_reset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 521\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_next_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    522\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_num_yielded\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    523\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dataset_kind\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0m_DatasetKind\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mIterable\u001B[0m \u001B[0;32mand\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/environment/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m_next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1184\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1185\u001B[0m             \u001B[0;32massert\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_shutdown\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_tasks_outstanding\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1186\u001B[0;31m             \u001B[0midx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_get_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1187\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_tasks_outstanding\u001B[0m \u001B[0;34m-=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1188\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dataset_kind\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0m_DatasetKind\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mIterable\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/environment/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m_get_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1150\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1151\u001B[0m             \u001B[0;32mwhile\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1152\u001B[0;31m                 \u001B[0msuccess\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_try_get_data\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1153\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0msuccess\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1154\u001B[0m                     \u001B[0;32mreturn\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/environment/miniconda3/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m_try_get_data\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    988\u001B[0m         \u001B[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    989\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 990\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_data_queue\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    991\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    992\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/environment/miniconda3/lib/python3.7/multiprocessing/queues.py\u001B[0m in \u001B[0;36mget\u001B[0;34m(self, block, timeout)\u001B[0m\n\u001B[1;32m    111\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_rlock\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrelease\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    112\u001B[0m         \u001B[0;31m# unserialize the data after having released the lock\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 113\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0m_ForkingPickler\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloads\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mres\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    114\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    115\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mqsize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/environment/miniconda3/lib/python3.7/site-packages/torch/multiprocessing/reductions.py\u001B[0m in \u001B[0;36mrebuild_storage_fd\u001B[0;34m(cls, df, size)\u001B[0m\n\u001B[1;32m    287\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    288\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mrebuild_storage_fd\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcls\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msize\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 289\u001B[0;31m     \u001B[0mfd\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdetach\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    290\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    291\u001B[0m         \u001B[0mstorage\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstorage_from_cache\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcls\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfd_id\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfd\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/environment/miniconda3/lib/python3.7/multiprocessing/resource_sharer.py\u001B[0m in \u001B[0;36mdetach\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     55\u001B[0m         \u001B[0;32mdef\u001B[0m \u001B[0mdetach\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     56\u001B[0m             \u001B[0;34m'''Get the fd.  This should only be called once.'''\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 57\u001B[0;31m             \u001B[0;32mwith\u001B[0m \u001B[0m_resource_sharer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_connection\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_id\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mconn\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     58\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mreduction\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrecv_handle\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     59\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/environment/miniconda3/lib/python3.7/multiprocessing/resource_sharer.py\u001B[0m in \u001B[0;36mget_connection\u001B[0;34m(ident)\u001B[0m\n\u001B[1;32m     85\u001B[0m         \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mconnection\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mClient\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     86\u001B[0m         \u001B[0maddress\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkey\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mident\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 87\u001B[0;31m         \u001B[0mc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mClient\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0maddress\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mauthkey\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mprocess\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcurrent_process\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mauthkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     88\u001B[0m         \u001B[0mc\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkey\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetpid\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     89\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mc\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/environment/miniconda3/lib/python3.7/multiprocessing/connection.py\u001B[0m in \u001B[0;36mClient\u001B[0;34m(address, family, authkey)\u001B[0m\n\u001B[1;32m    496\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    497\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mauthkey\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 498\u001B[0;31m         \u001B[0manswer_challenge\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mauthkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    499\u001B[0m         \u001B[0mdeliver_challenge\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mauthkey\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    500\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/environment/miniconda3/lib/python3.7/multiprocessing/connection.py\u001B[0m in \u001B[0;36manswer_challenge\u001B[0;34m(connection, authkey)\u001B[0m\n\u001B[1;32m    740\u001B[0m         raise ValueError(\n\u001B[1;32m    741\u001B[0m             \"Authkey must be bytes, not {0!s}\".format(type(authkey)))\n\u001B[0;32m--> 742\u001B[0;31m     \u001B[0mmessage\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconnection\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrecv_bytes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m256\u001B[0m\u001B[0;34m)\u001B[0m         \u001B[0;31m# reject large message\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    743\u001B[0m     \u001B[0;32massert\u001B[0m \u001B[0mmessage\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mCHALLENGE\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mCHALLENGE\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'message = %r'\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mmessage\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    744\u001B[0m     \u001B[0mmessage\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmessage\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mCHALLENGE\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/environment/miniconda3/lib/python3.7/multiprocessing/connection.py\u001B[0m in \u001B[0;36mrecv_bytes\u001B[0;34m(self, maxlength)\u001B[0m\n\u001B[1;32m    214\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mmaxlength\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mmaxlength\u001B[0m \u001B[0;34m<\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    215\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"negative maxlength\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 216\u001B[0;31m         \u001B[0mbuf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_recv_bytes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmaxlength\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    217\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mbuf\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    218\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_bad_message_length\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/environment/miniconda3/lib/python3.7/multiprocessing/connection.py\u001B[0m in \u001B[0;36m_recv_bytes\u001B[0;34m(self, maxsize)\u001B[0m\n\u001B[1;32m    405\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    406\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_recv_bytes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmaxsize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 407\u001B[0;31m         \u001B[0mbuf\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_recv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m4\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    408\u001B[0m         \u001B[0msize\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mstruct\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0munpack\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"!i\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mbuf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetvalue\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    409\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mmaxsize\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0msize\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0mmaxsize\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/environment/miniconda3/lib/python3.7/multiprocessing/connection.py\u001B[0m in \u001B[0;36m_recv\u001B[0;34m(self, size, read)\u001B[0m\n\u001B[1;32m    377\u001B[0m         \u001B[0mremaining\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msize\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    378\u001B[0m         \u001B[0;32mwhile\u001B[0m \u001B[0mremaining\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 379\u001B[0;31m             \u001B[0mchunk\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhandle\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mremaining\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    380\u001B[0m             \u001B[0mn\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mchunk\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    381\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mn\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "runner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "MMSegmentation Tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "20d4b83e0c8b3730b580c42434163d64f4b735d580303a8fade7c849d4d29eba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
